{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "rureviews = pd.read_csv(\"drive/MyDrive/Colab Notebooks/women-clothing-accessories.csv\", sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMHE6RHiWOQh",
        "outputId": "2c57b007-2031-4dff-ff1c-6de507f0de35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов.\n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать гиперпараметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор.\n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer() # лемматизатор"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lynrs4OCcCdq",
        "outputId": "51e8b558-2923-46c6-aefb-ddf825ccf97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6634a8f53af0c2e0195936528963ff213c735217ca1e3138a4c481b3c170fcad\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk\n",
        "nltk.download('stopwords') # импорт стоп-слов\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "\n",
        "stop_words = stopwords.words('russian') # стоп-слова для русского языка\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower()) # приведение к нижнему регистру\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation] # удаление стоп-слов и ненужной пунктуации\n",
        "    tokens = [pymorphy2_analyzer.parse(token)[0].normal_form for token in tokens] # лемматизация\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "rureviews['review_preprocessed'] = rureviews['review'].apply(preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzDU76xMX7Wi",
        "outputId": "88465c39-1066-4597-8fa4-1c04c4ff12f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB # наивный байесовский классификатор\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(rureviews.review_preprocessed, rureviews.sentiment, train_size = 0.7)"
      ],
      "metadata": {
        "id": "H3CRxdh5etDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка векторайзеров\n",
        "\n",
        "# Мешок n-грамм\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def select_ngram_range(corpus, result, min_n, max_n):\n",
        "    best_ngram_range = (min_n, min_n)\n",
        "    best_score = 0\n",
        "    for n in range(min_n, max_n+1):\n",
        "        for m in range(n+1, max_n+1):\n",
        "            vectorizer = CountVectorizer(ngram_range=(n, m))\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        "            bayes = MultinomialNB().fit(X, result)\n",
        "            prediction = bayes.predict(X)\n",
        "            score = accuracy_score(result, prediction)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_ngram_range = (n, m)\n",
        "\n",
        "\n",
        "    return best_ngram_range\n",
        "best_n = select_ngram_range(x_train, y_train, 1, 5)\n",
        "print(best_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX9p2SXiK7Nd",
        "outputId": "143d5a96-7a4e-41e6-9655-c9f8f684bc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_n = (1, 5)"
      ],
      "metadata": {
        "id": "t-b6Eb0S8n95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(ngram_range=best_n)\n",
        "count_vectorised_x_train = vectorizer.fit_transform(x_train)\n",
        "count_vectorised_x_test = vectorizer.transform(x_test)\n",
        "bayes_for_count_vectoriser = MultinomialNB().fit(count_vectorised_x_train, y_train)\n",
        "count_vectoriser_prediction = bayes_for_count_vectoriser.predict(count_vectorised_x_test)"
      ],
      "metadata": {
        "id": "oVqcXcJHXaBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def select_params(corpus, result, min_n, max_n):\n",
        "    best_ngram_range = (min_n, min_n)\n",
        "    best_score = 0\n",
        "    for n in range(min_n, max_n+1):\n",
        "        for m in range(n+1, max_n+1):\n",
        "          for maxdf in np.linspace(0.1, 1, 3):\n",
        "            for mindf in np.linspace(0.1, 1, 3):\n",
        "              try:\n",
        "                  vectorizer = TfidfVectorizer(ngram_range=(n, m), max_df=maxdf, min_df=mindf)\n",
        "                  X = vectorizer.fit_transform(corpus)\n",
        "                  bayes = MultinomialNB().fit(X, result)\n",
        "                  prediction = bayes.predict(X)\n",
        "                  score = accuracy_score(result, prediction)\n",
        "\n",
        "              except ValueError:\n",
        "                  score = 0\n",
        "              if score > best_score:\n",
        "                  best_score = score\n",
        "                  best_ngram_range = (n, m)\n",
        "                  best_max_df = maxdf\n",
        "                  best_min_df = mindf\n",
        "\n",
        "    return best_ngram_range, best_max_df, best_min_df, 10\n",
        "best_n, max_df, min_df, max_features = select_params(x_train, y_train, 1, 5)"
      ],
      "metadata": {
        "id": "g2XHXZQWY0Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_n, max_df, min_df, max_features = (1, 5), 0.65, 0.0, 190"
      ],
      "metadata": {
        "id": "PRa4ntY58T9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_n, max_df, min_df, max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUJzsU3h1gWQ",
        "outputId": "fcf4f8e6-ba57-4eb7-b527-bf0296c645c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5) 0.65 0.0 190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=best_n, max_df=max_df, min_df=min_df, max_features=max_features)\n",
        "tfidf_vectorised_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "tfidf_vectorised_x_test = tfidf_vectorizer.transform(x_test)\n",
        "bayes_for_tfidf_vectoriser = MultinomialNB().fit(tfidf_vectorised_x_train, y_train)\n",
        "tfidf_vectoriser_prediction = bayes_for_tfidf_vectoriser.predict(tfidf_vectorised_x_test)"
      ],
      "metadata": {
        "id": "XAFQ6kUlZXwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Символьная n-грамма\n",
        "\n",
        "def select_ngram_range(corpus, result, min_n, max_n):\n",
        "    best_ngram_range = (min_n, min_n)\n",
        "    best_score = 0\n",
        "    for n in range(min_n, max_n+1):\n",
        "        for m in range(n+1, max_n+1):\n",
        "            vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, m))\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        "            bayes = MultinomialNB().fit(X, result)\n",
        "            prediction = bayes.predict(X)\n",
        "            score = accuracy_score(result, prediction)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_ngram_range = (n, m)\n",
        "\n",
        "\n",
        "    return best_ngram_range\n",
        "best_n = select_ngram_range(x_train, y_train, 1, 5)\n",
        "best_n = (4, 5)\n",
        "print(best_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qrfpS4a2_T_",
        "outputId": "44323a11-4764-4846-8740-74b6d2305569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=best_n)\n",
        "char_count_vectorised_x_train = char_vectorizer.fit_transform(x_train)\n",
        "char_count_vectorised_x_test = char_vectorizer.transform(x_test)\n",
        "bayes_for_char_count_vectoriser = MultinomialNB().fit(char_count_vectorised_x_train, y_train)\n",
        "char_count_vectoriser_prediction = bayes_for_char_count_vectoriser.predict(char_count_vectorised_x_test)"
      ],
      "metadata": {
        "id": "8n9WLdeA3ZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(columns=['vectoriser_name', 'precision', 'recall', 'f1_score', 'accuracy'])\n",
        "results.loc[len(results.index)] = [\n",
        "                'CountVectoriser',\n",
        "                precision_score(y_test, count_vectoriser_prediction, average='weighted'),\n",
        "                recall_score(y_test, count_vectoriser_prediction, average='weighted'),\n",
        "                f1_score(y_test, count_vectoriser_prediction, average='weighted'),\n",
        "                accuracy_score(y_test, count_vectoriser_prediction)]\n",
        "results.loc[len(results.index)] = ['TfIdf',\n",
        "                precision_score(y_test, tfidf_vectoriser_prediction, average='weighted'),\n",
        "                recall_score(y_test, tfidf_vectoriser_prediction, average='weighted'),\n",
        "                f1_score(y_test, tfidf_vectoriser_prediction, average='weighted'),\n",
        "                accuracy_score(y_test, tfidf_vectoriser_prediction)]\n",
        "results.loc[len(results.index)] = ['CharCountVectoriser',\n",
        "                precision_score(y_test, char_count_vectoriser_prediction, average='weighted'),\n",
        "                recall_score(y_test, char_count_vectoriser_prediction, average='weighted'),\n",
        "                f1_score(y_test, char_count_vectoriser_prediction, average='weighted'),\n",
        "                accuracy_score(y_test, char_count_vectoriser_prediction)]\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bif-pcU24eTn",
        "outputId": "5db71c64-a92c-4b61-8dd5-b0005c281567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       vectoriser_name  precision    recall  f1_score  accuracy\n",
            "0      CountVectoriser   0.716476  0.716899  0.716511  0.716899\n",
            "1                TfIdf   0.672747  0.661050  0.663318  0.661050\n",
            "2  CharCountVectoriser   0.712240  0.701678  0.703751  0.701678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах.\n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **а?** - ноль или один символ **а**\n",
        "* **а+** - один или более символов **а**\n",
        "* **а\\*** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.findall('a?b.', 'aabbсabbcbb')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Wh4EZcUVWB",
        "outputId": "ab4ee9be-c3f4-4b1a-d514-4af33f693cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abb', 'abb', 'bb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.findall('a*b.', 'aabbсabbcbb')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN7gjUwWGON",
        "outputId": "02552bb7-724e-44c7-8d27-980ce9e6fe28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aabb', 'abb', 'bb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.findall('a+b.', 'aabbсabbcbb')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "izXJKMwJWKyY",
        "outputId": "dc22cce1-3bdb-4fef-e412-4f1a4ad3da7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aabb', 'abb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**:\n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53539cd1-39a7-42bd-9292-8adb3b6a0b52"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?\n",
        "Часть входит в уже найденный 'abca'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bfc61d-e317-4b9a-9b4d-cdd54b1b2e44"
      },
      "source": [
        "text = \"When the Fox hears the Rabbit scream he comes a-runnin', but not to help.\"\n",
        "result = re.findall(r'\\b[\\w-]{2}', re.sub('-', \"\", text))\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wh', 'th', 'Fo', 'he', 'th', 'Ra', 'sc', 'he', 'co', 'ar', 'bu', 'no', 'to', 'he']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b5d289-8e8c-4621-fe3c-34aa130c727c"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45699604-9950-4185-cd4f-0e3ad6655629"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d129f5-4975-41b0-c0dd-bf384c6d62f2"
      },
      "source": [
        "text = \"Clarice Starling flinched as the first of the heavy steel gates clashed shut behind her and the bolt shot home. Chilton walked slightly ahead, down the green institutional corridor in an atmosphere of Lysol and distant slammings. Starling was angry at herself for letting Chilton put his hand in her purse and briefcase, and she stepped hard on the anger so that she could concentrate. It was all right. She felt her control solid beneath her, like a good gravel bottom in a fast current. \"\n",
        "result = re.split('\\. ', text, maxsplit=2)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Clarice Starling flinched as the first of the heavy steel gates clashed shut behind her and the bolt shot home', 'Chilton walked slightly ahead, down the green institutional corridor in an atmosphere of Lysol and distant slammings', 'Starling was angry at herself for letting Chilton put his hand in her purse and briefcase, and she stepped hard on the anger so that she could concentrate. It was all right. She felt her control solid beneath her, like a good gravel bottom in a fast current. ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f260be-ce79-405c-b21a-5ca8345ff4b7"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4752c262-2667-439d-efeb-dca9a9f0bb32"
      },
      "source": [
        "text = \"On March 22, 1975, he failed to appear for a performance in Baltimore. On March 25 his body was discovered seated in a pew in a small rural church \"\n",
        "result = re.sub(r'\\d', 'DIG', text)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On March DIGDIG, DIGDIGDIGDIG, he failed to appear for a performance in Baltimore. On March DIGDIG his body was discovered seated in a pew in a small rural church \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772fb563-d521-43d8-b955-50eb576c4146"
      },
      "source": [
        "text = \"Пример URL адреса: https://en.wikipedia.org/wiki/URL#:~:text=Most%20web%20browsers%20display%20the,name%20(%20index.html%20.\"\n",
        "result = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+.*[^.]\", \"\", text)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример URL адреса: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60e13a6-9d93-46e5-f329-620f6072715f"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ad06e3-79bc-42fb-9fc9-d3295fa53d29"
      },
      "source": [
        "word_finder = re.compile(r'\\b\\w{4,}\\b')\n",
        "print(word_finder.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Слова', 'больше', 'больше', 'слов']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\"\n",
        "adresses = re.compile(r'@\\S+(?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b')\n",
        "adresses.findall(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_EmTmvwedg",
        "outputId": "2e5173c4-6083-416f-c16e-fa2600d70ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    }
  ]
}