{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGClrhQA9SAk"
      },
      "source": [
        "# Деревья решений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veekMy8WRjBi"
      },
      "source": [
        "## Построение дерева"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkVwAFiUHXj"
      },
      "source": [
        "Опишем жадный алгоритм построения бинарного дерева решений:\n",
        "1. Начинаем со всей обучающей выборки $X$, которую помещаем в корень $R_1$.\n",
        "2. Задаём функционал качества $Q(X, j, t)$ и критерий остановки.\n",
        "3. Запускаем построение из корня: $SplitNode(1, R_1)$\n",
        "\n",
        "Функция $SplitNode(m, R_m)$\n",
        "1. Если выполнен критерий остановки, то выход.\n",
        "2. Находим наилучший с точки зрения $Q$ предикат: $j, t$: $[x_j<t]$\n",
        "3. Помещаем предикат в вершину и получаем с его помощью разбиение $X$ на две части: $R_{left} = \\lbrace x|x_j<t \\rbrace$ и $R_{right} = \\lbrace x|x_j \\geqslant t \\rbrace$\n",
        "4. Поместим $R_{left}$ и $R_{right}$ соответсвенно в левое и правое поддерево.\n",
        "5. Рекурсивно повторяем $SplitNode(left, R_{left})$ и $SplitNode(right, R_{right})$.\n",
        "\n",
        "В конце поставим в соответствие каждому листу ответ. Для задачи классификации - это самый частый среди объектов класс или вектор с долями классов (можно интерпретировать как вероятности):\n",
        "$$ c_v = \\arg \\max_{k\\in Y} \\sum_{(x_i,y_i) \\in R_v} [y_i=k]  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6FsdBog4Ai"
      },
      "source": [
        "## Функционал качества для деревьев решений\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAKO0aykGBD"
      },
      "source": [
        "Энтропия Шеннона для системы с N возможными состояниями определяется по формуле:\n",
        "$$H = - \\sum_{i=0}^{N} p_i\\log_2p_i $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5582B-1Fn2bw"
      },
      "source": [
        "где $p_i$ – вероятности нахождения системы в $i$-ом состоянии.\n",
        "\n",
        "Это очень важное понятие теории информации, которое позволяет оценить количество информации (степень хаоса в системе). Чем выше энтропия, тем менее упорядочена система и наоборот. С помощью энтропии мы формализуем функционал качества для разделение выборки (для задачи классификации)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbcMUd7bvk05"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdLxP9CowTm"
      },
      "source": [
        "Код для расчёта энтропии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mT8Jq8Av2sM"
      },
      "source": [
        "def entropy(y):\n",
        "\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "    return entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk9etb2vo7fK"
      },
      "source": [
        "Здесь $y$ - это массив значений целевой переменной"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TCw0USzLus"
      },
      "source": [
        "Энтропия – по сути степень хаоса (или неопределенности) в системе. Уменьшение энтропии называют приростом информации (information gain, IG).\n",
        "\n",
        "Обозначим $R_v$ - объекты, которые нужно разделить в помощью предиката в вершине $v$. Запишем формулу для расчёта информационного прироста:\n",
        "$$ Q = IG = H(R_v) - (H(R_{left})+H(R_{right}))$$\n",
        "\n",
        "На каждом шаге нам нужно максимизировать этот функционал качества. Как это делать? Например, так можно перебрать $t$ для выбранного $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trEWHDoXg_p9"
      },
      "source": [
        "Предыдущая версия формулы прироста информации слишком упрощена. В работе необходимо использовать более устойчивую формулу, которая учитывает не только энтропию подмножеств, но и их размер.\n",
        "\n",
        "$$ Q = IG = H(R_v) - \\Big (\\frac{|R_{left}|} {|R_{v}|} H(R_{left})+ \\frac{|R_{right}|} {|R_{v}|} H(R_{right})\\Big)$$\n",
        "\n",
        "где, $|R_{v}|$, $|R_{left}|$ и $|R_{right}|$ - количество элементов в соответствующих множествах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xmN6V_N1xBr"
      },
      "source": [
        "\n",
        "### Задание 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWFHZScF2CBF"
      },
      "source": [
        "Реализуйте алгоритм построения дерева. Должны быть отдельные функции (методы) для расчёта энтропии (уже есть), для разделения узлов дерева (используйте, например, `pandas`), для подсчёта функционала качества $IG$, для выбора наилучшего разделения (с учетом признаков и порогов), для проверки критерия остановки.\n",
        "\n",
        "Для набора данных `iris` реализуйте алгоритм и минимум три разных критерия остановки из перечисленных ниже:\n",
        "* максимальной глубины дерева = 5\n",
        "* минимального числа объектов в листе = 5\n",
        "* максимальное количество листьев в дереве = 5\n",
        "* <underline> purity (остановка, если все объекты в листе относятся к одному классу) </underline>\n",
        "\n",
        "Реализуйте функцию `predict` (на вход функции подаётся датафрейм с объектами)\n",
        "\n",
        "Оцените точность каждой модели с помощью метрики доля правильных ответов (`from sklearn.metrics import accuracy_score` или реализовать свою)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self, min_samples_split=None, max_depth=None, max_leaves_num=None, purity=True):\n",
        "      # Критерии остановки\n",
        "      self.min_samples_split = min_samples_split\n",
        "      self.max_depth = max_depth\n",
        "      self.max_leaves_num = max_leaves_num\n",
        "      self.purity = purity\n",
        "\n",
        "      self.leaves_count = 0\n",
        "      self.root = None\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "      counter = Counter(y)\n",
        "      return counter.most_common(1)[0][0]\n",
        "\n",
        "  def _split(self, X_column, threshold):\n",
        "      left_idxs = np.argwhere(X_column < threshold).flatten()\n",
        "      right_idxs = np.argwhere(X_column >= threshold).flatten()\n",
        "\n",
        "      return left_idxs, right_idxs\n",
        "\n",
        "  # вычисление информационного прироста\n",
        "  def _information_gain(self, y, X_column, threshold):\n",
        "      parent_entropy = entropy(y)\n",
        "\n",
        "      left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "      if (len(left_idxs) == 0 or len(right_idxs == 0)):\n",
        "          return 0;\n",
        "\n",
        "      n = len(y)\n",
        "      n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "      e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "      child_entropy = (n_l / n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "      return parent_entropy - child_entropy\n",
        "\n",
        "  def _best_split(self, X, y, feat_idxs):\n",
        "      best_gain = -1\n",
        "      split_idx, split_treshold = None, None\n",
        "\n",
        "      for feat_idx in feat_idxs:\n",
        "        X_column = X[:, feat_idx]\n",
        "        thresholds = np.unique(X_column)\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            gain = self._information_gain(y, X_column, threshold)\n",
        "            if (gain > best_gain):\n",
        "                best_gain = gain\n",
        "                split_idx = feat_idx\n",
        "                split_threshold = threshold\n",
        "      return split_idx, split_treshold\n",
        "\n",
        "  def _grow_tree(self, X, y, depth=0):\n",
        "      n_samples, n_features = X.shape\n",
        "      n_labels = len(np.unique(y))\n",
        "\n",
        "      if (((depth >= self.max_depth) if (self.max_depth is not None) else False) or\n",
        "          ((n_labels == 1) if (self.purity == True) else False)or\n",
        "          ((n_samples < self.min_samples_split) if (self.min_samples_split is not None) else False)):\n",
        "\n",
        "          leaf_value = self._most_common_label(y)\n",
        "\n",
        "          return Node(value=leaf_value)\n",
        "\n",
        "      feat_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
        "\n",
        "      best_feature, best_threshold = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "      left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "      left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "      right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "\n",
        "      return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      self.n_features = X.shape[1]\n",
        "      self.root = self._grow_tree(X, y)\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "      if (node.is_leaf_node()):\n",
        "          return node.value\n",
        "\n",
        "      if x[node.feature] < node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "      return self._traverse_tree(x, node.right)\n",
        "\n",
        "  def predict(self, X):\n",
        "      return np.array([self._traverse_tree(x, self.root) for x in X])\n"
      ],
      "metadata": {
        "id": "Kmu6PgKWJZ1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=None, max_depth=None, max_leaves_num=None, purity=True):\n",
        "        # Критерии остановки\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.max_leaves_num = max_leaves_num\n",
        "        self.purity = purity\n",
        "\n",
        "        self.root = None\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    # вычисление информационного прироста\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        parent_entropy = entropy(y)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        n = len(y)\n",
        "        left_len, right_len = len(left_idxs), len(right_idxs)\n",
        "        left_entropy, right_entropy = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "        child_entropy = (left_len / n) * left_entropy + (right_len / n) * right_entropy\n",
        "\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if (((depth >= self.max_depth) if (self.max_depth is not None) else False) or\n",
        "           ((n_labels == 1) if (self.purity == True) else (len(y) >= 1)) or\n",
        "           ((n_samples < self.min_samples_split) if (self.min_samples_split is not None) else False)):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # разделение на ветви\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n"
      ],
      "metadata": {
        "id": "VVmtyPe8i251"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sys\n",
        "sys.setrecursionlimit(100000)\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "decisionTree_max_depth = DecisionTree(max_depth=5)\n",
        "decisionTree_max_depth.fit(X_train, y_train)\n",
        "predictions_max_depth = decisionTree_max_depth.predict(X_test)\n",
        "accuracy_max_depth = accuracy_score(y_test, predictions_max_depth)\n",
        "\n",
        "decisionTree_max_leaves = DecisionTree(max_leaves_num=5)\n",
        "decisionTree_max_leaves.fit(X_train, y_train)\n",
        "predictions_max_leaves = decisionTree_max_leaves.predict(X_test)\n",
        "accuracy_max_leaves = accuracy_score(y_test, predictions_max_leaves)\n",
        "\n",
        "decisionTree_min_samples_split = DecisionTree(min_samples_split=5)\n",
        "decisionTree_min_samples_split.fit(X_train, y_train)\n",
        "predictions_min_samples_split = decisionTree_min_samples_split.predict(X_test)\n",
        "accuracy_min_samples_split = accuracy_score(y_test, predictions_min_samples_split)\n",
        "\n",
        "decisionTree_purity = DecisionTree(purity=True)\n",
        "decisionTree_purity.fit(X_train, y_train)\n",
        "predictions_purity = decisionTree_purity.predict(X_test)\n",
        "accuracy_purity = accuracy_score(y_test, predictions_purity)\n",
        "\n",
        "from tabulate import tabulate\n",
        "table = [[' ', 'max_depth', 'max_leaves', 'min_samples', 'purity'],\n",
        "         ['Accuracy', accuracy_max_depth, accuracy_max_leaves, accuracy_min_samples_split, accuracy_purity]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2UA4jU-olbq",
        "outputId": "97fa8333-61f2-4358-a13e-e0b42a77c46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒══════════╤═════════════╤══════════════╤═══════════════╤══════════╕\n",
            "│          │   max_depth │   max_leaves │   min_samples │   purity │\n",
            "╞══════════╪═════════════╪══════════════╪═══════════════╪══════════╡\n",
            "│ Accuracy │        0.96 │         0.98 │          0.96 │     0.98 │\n",
            "╘══════════╧═════════════╧══════════════╧═══════════════╧══════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyCjLcy_CTM"
      },
      "source": [
        "##  Случайный лес"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKZe1FyRgCa"
      },
      "source": [
        "Опишем алгоритм случайный лес (*random forest*) и попутно разберём основные идеи:\n",
        "\n",
        "1. Зададим $N$ - число деревьев в лесу.\n",
        "2. Для каждого $n$ из $N$ сгенерируем свою выборку $X_n$. Пусть $m$ - это количество объектов в $X$. При генерации каждой $X_n$ мы будем брать объекты $m$ раз с возвращением. То есть один и тот же объект может попасть в выборку несколько раз, а какие-то объекты не попадут. (Этот способ назвается бутстрап).\n",
        "3. По каждой $X_n$ построим решающее дерево $b_n$. Обычно стараются делать глубокие деревья. В качестве критериев остановки можно использовать `max_depth` или `min_samples_leaf` (например, пока в каждом листе не окажется по одному объекту). При каждом разбиении сначала выбирается $k$ (эвристика $k = \\sqrt d$, где $d$ - это число признаков объектов из выборки $X$) случайных признаков из исходных, и оптимальное разделение выборки ищется только среди них. Обратите внимание, что мы не выбрасываем оставшиеся признаки!\n",
        "4. Итоговый алгоритм будет представлять собой результат голосования (для классификации) и среднее арифметическое (для регрессии). Модификация алгоритма предполагает учёт весов каждого отдельного слабого алгоритма в ансамбле, но в этом особо нет смысла.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import math\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10,*, max_tree_depth=None, min_tree_samples_split=2, features_num=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_tree_depth = max_tree_depth\n",
        "        self.min_tree_samples_split = min_tree_samples_split\n",
        "        self.features_num = features_num #k\n",
        "\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if self.features_num is None:\n",
        "                        self.features_num = math.ceil(math.sqrt(n_features))\n",
        "\n",
        "        for i in range(self.n_trees):\n",
        "            #decisionTree = DecisionTree(min_samples_split=self.min_tree_samples_split, max_depth=self.max_tree_depth)\n",
        "            decisionTree = DecisionTreeClassifier(criterion='entropy',\n",
        "                                                  max_depth=self.max_tree_depth,\n",
        "                                                  min_samples_split=self.min_tree_samples_split,\n",
        "                                                  max_features=self.features_num)\n",
        "\n",
        "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "\n",
        "            X_train, y_train = X.iloc[idxs], y.iloc[idxs]\n",
        "\n",
        "            decisionTree.fit(X_train, y_train)\n",
        "\n",
        "            self.trees.append(decisionTree)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        predictions = np.swapaxes(predictions, 0, 1)\n",
        "        return np.array([self._most_common_label(prediction) for prediction in predictions])"
      ],
      "metadata": {
        "id": "nRZkMd0IGZOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBQ8lc0WyrN"
      },
      "source": [
        "### Задание 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y594Jn04ZTCm"
      },
      "source": [
        "В качестве набора данных используйте: https://www.kaggle.com/mathchi/churn-for-bank-customers\n",
        "\n",
        "Там есть описание и примеры работы с этими данными. Если кратко, речь идёт про задачу прогнозирования оттока клиентов. Есть данные о 10 тысячах клиентов банка, часть из которых больше не являются клиентами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_mLbdVW2oG"
      },
      "source": [
        "Используя либо свою реализацию, либо  `DecisionTreeClassifier` с разными настройками из `sklearn.tree` реализйте алгоритм \"случайный лес\".\n",
        "\n",
        "Найдите наилучшие гиперпараметры этого алгоритма: количество деревьев, критерий остановки, функционал качества, минимальное количество объектов в листьях и другие.\n",
        "\n",
        "Нельзя использовать готовую реализацию случайного леса из `sklearn`.\n",
        "\n",
        "В подобных задачах очень важна интерпретируемость алгоритма. Попытайтесь оценить информативность признаков, т.е. ответить а вопрос, значения каких признаков являются самыми важными индикаторами того, что банк потеряет клиента."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv(\"drive/MyDrive/Colab Notebooks//churn.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17YRgCSoVdH4",
        "outputId": "4668df6f-ac20-4448-fbc0-9c60be42a3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilwMarXhWlWL",
        "outputId": "27a874be-08a7-4883-8058-d23a0b3282b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
            "0             1    15634602   Hargrave          619    France  Female   42   \n",
            "1             2    15647311       Hill          608     Spain  Female   41   \n",
            "2             3    15619304       Onio          502    France  Female   42   \n",
            "3             4    15701354       Boni          699    France  Female   39   \n",
            "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
            "...         ...         ...        ...          ...       ...     ...  ...   \n",
            "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
            "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
            "9997       9998    15584532        Liu          709    France  Female   36   \n",
            "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
            "9999      10000    15628319     Walker          792    France  Female   28   \n",
            "\n",
            "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
            "0          2       0.00              1          1               1   \n",
            "1          1   83807.86              1          0               1   \n",
            "2          8  159660.80              3          1               0   \n",
            "3          1       0.00              2          0               0   \n",
            "4          2  125510.82              1          1               1   \n",
            "...      ...        ...            ...        ...             ...   \n",
            "9995       5       0.00              2          1               0   \n",
            "9996      10   57369.61              1          1               1   \n",
            "9997       7       0.00              1          0               1   \n",
            "9998       3   75075.31              2          1               0   \n",
            "9999       4  130142.79              1          1               0   \n",
            "\n",
            "      EstimatedSalary  Exited  \n",
            "0           101348.88       1  \n",
            "1           112542.58       0  \n",
            "2           113931.57       1  \n",
            "3            93826.63       0  \n",
            "4            79084.10       0  \n",
            "...               ...     ...  \n",
            "9995         96270.64       0  \n",
            "9996        101699.77       0  \n",
            "9997         42085.58       1  \n",
            "9998         92888.52       1  \n",
            "9999         38190.78       0  \n",
            "\n",
            "[10000 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = data.drop([\"RowNumber\", \"Surname\", \"CustomerId\"], axis=1)\n",
        "data['Gender'] = data['Gender'].replace({'Male': 0, 'Female': 1})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['Geography'] = label_encoder.fit_transform(data['Geography'])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"Exited\"], axis=1, inplace=False),\n",
        "                                                    data[\"Exited\"],\n",
        "                                                    train_size=0.7)"
      ],
      "metadata": {
        "id": "hrtgA_pmD62s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trees_amounts = [5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
        "min_samples_splits = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "max_depths = [5, 10, 15, 20, 25, 30]\n",
        "features_amounts = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "best_accuracy = -1\n",
        "best_forest = None\n",
        "for n_features in features_amounts:\n",
        "  for trees_num in trees_amounts:\n",
        "    for min_samples_split in min_samples_splits:\n",
        "      for max_depth in max_depths:\n",
        "        forest = RandomForest(n_trees=trees_num, max_tree_depth=max_depth, min_tree_samples_split=min_samples_split, features_num=n_features)\n",
        "        forest.fit(X_train, y_train)\n",
        "        prediction = forest.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, prediction)\n",
        "        if accuracy > best_accuracy:\n",
        "          best_accuracy = accuracy\n",
        "          best_forest = forest\n",
        "          best_params = {\n",
        "              'n_trees' : trees_num,\n",
        "              'min_tree_samples_split' : min_samples_split,\n",
        "              'max_depth' : max_depth,\n",
        "              'features_num' : n_features\n",
        "          }\n",
        "print(best_accuracy, best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGzN8lNX-ftI",
        "outputId": "9f340e02-9e9c-41fa-b3ac-fa15565ef26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8713333333333333 {'n_trees': 45, 'min_tree_samples_split': 10, 'max_depth': 10, 'features_num': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testTree = DecisionTree(max_depth = 10, min_samples_split=10) # Эталонное дерево\n",
        "testTree.fit(X_train, y_train)\n",
        "# внутренний обход дерева для выяснения важности признаков\n",
        "root = testTree.root\n",
        "features_in_order = []\n",
        "def inner_traverse(tnode):\n",
        "    if (tnode.feature is None):\n",
        "      return\n",
        "\n",
        "    features_in_order.append(tnode.feature)\n",
        "    inner_traverse(tnode.left)\n",
        "    inner_traverse(tnode.right)\n",
        "inner_traverse(root)\n",
        "print('Топ 3 признака по важности')\n",
        "for feature in list(dict.fromkeys(features_in_order)):\n",
        "  print(data.columns.values.tolist()[feature])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcKlJA9kwU6o",
        "outputId": "69e694cf-4dd2-4ca6-c276-6c911c9074fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ 3 признака по важности\n",
            "Gender\n",
            "Age\n",
            "Geography\n"
          ]
        }
      ]
    }
  ]
}